{
  "hash": "bea49c526642e9b17ce462fd6752b530",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Remembering what I did this week with LLMs\ndescription: >\n  Using the GitHub API and `{ellmer}` to remember what I did this week.\ncategories:\n  - R\n  - LLMs\n  - Personal Data\n  - Productivity\n  - GitHub\n  - ellmer\nimage: calendar-scribbles.png\nimage-alt: >\n  A generative AI image of a wall calendar behind a desk. The calendar is a mess.\ndate: last-modified\n\neditor: {render-on-save: true}\n---\n\n::: {.cell}\n\n:::\n\n\n\n## The object impermanence of work\n\nMy least favorite meetings are status meetings.\nI love seeing the people I work with and hearing about what they've been up to,\nbut I hate trying to remember what the hell I've been working on.\n\nWhen I joined the Shiny team,\nwe'd have a short daily status meeting\nand you'd think it'd be easy to remember what I did in the last 24 hours.\nBut apparently, that's not how I live my life.\nI'm sure I thought of things to say,\nbut every daily update I felt like I'd say some variation of\n\"I did a bunch of stuff and now I'm... doing more stuff\".\n\nMy perception of how I work is in short bursts of deep focus time\npunctuated by many short breaks to answer questions\non Slack or GitHub issues\nor [the Shiny Discord server](https://discord.gg/yMGCamUMnS)\nor [Posit Community](https://community.posit.co)\nor any of the many other places that people post questions and sometimes code.\n\nBetween the deep focus work,\nI usually maintain one thread of maintenance tasks,\nlike babysitting a CI process,\nor making a small change to fix a bug,\nor [turning screenshots into reproducible bug reports](https://bsky.app/profile/grrrck.xyz/post/3lgdolbiqis25).\nAnd I also tend to have another more speculative thread going\nfor learning something new or playing with a new idea.\n\nI don't recommend my way of working to others.\nWhole books have been written just to say \"don't work like this\"!\nSoftware engineers rail against\n[context switching](https://contextkeeper.io/blog/the-real-cost-of-an-interruption-and-context-switching/),\n[interruptions](https://blog.stackblitz.com/posts/flow-state/),\nand [multitasking](https://www.apa.org/topics/research/multitasking).\n\nI compensate for the known perils of my work style in two ways.\nFirst, even if I do a lot of different things throughout the day,\nthere's generally only ever _one task_ that I'm working on at any given moment.\nThat task gets all the focus.\n\nSecond, I try to leave context behind in my work.\nIf I'm working on a pull request,\nthat means submitting the PR with enough description that I could\n[forget that I even wrote the PR](https://bsky.app/profile/grrrck.xyz/post/3lfslcgnoh225)\nand most of what I'd need to pick it back up is there in the description or comments.\nIf I had some code in front of me when I found the issue or was testing the PR,\nthat code ends up in the description.\nIf the context is recoverable, it's at least not lost forever in the switching.\n\nBut these two strategies combine in a strange way,\nresulting in the **object impermanence of work**:\n_if it's not in front of me, it doesn't exist_.\n\nI just don't keep a running inventory of what I was working on or what I did;\nI know that I can find everything I need to know from the open GitHub issues or PRs.\nWhich then are summarized in stand ups as... _stuff_.\n\n## What did I even do this week?\n\nIt turns out I _can_ answer the question of what I did this week,\nas long as I'm allowed to run some R code.\n\n\n\n::: {.cell}\n\n:::\n\n\n\nSince my entire life is lived on GitHub,\nI can get a pretty good sense of what I've been working on\nby looking at the PRs and issues I opened and closed in a week.\nEven better, this information is readily obtained from GitHub with a search like\n[`` author:gadenbuie created:>2025-01-17T00:00:00Z ``](https://github.com/search?q=author:gadenbuie%20created:%3E2025-01-17T00:00:00Z) or\n[`` author:gadenbuie closed:>2025-01-17T00:00:00Z ``](https://github.com/search?q=author:gadenbuie%20closed:%3E2025-01-17T00:00:00Z).\nAnd I can create those search queries with R:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nauthor <- \"gadenbuie\"\ndate <- Sys.Date() - 7\nsearch_created <- sprintf(\"author:%s created:>%sT00:00:00Z\", author, date)\nsearch_closed <- sprintf(\"author:%s closed:>%sT00:00:00Z\", author, date)\n```\n:::\n\n\n\nBut of course I don't want to have to actually click around on GitHub to remember what I was doing.\nwhat I want is some rectangular data that I can work on in R.\n\nEnter the [`gh` package](https://gh.r-lib.org).\nIt's a package that lets you interact with the GitHub API.\nWe can use `gh` to run the closed and created search queries and collect all of the results.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gh)\n\ncreated <- gh(\"/search/issues\", q = search_created, .limit = Inf)\nclosed <- gh(\"/search/issues\", q = search_closed, .limit = Inf)\n```\n:::\n\n\n\nWhy not do this in one single API request?\nUnfortunately, GitHub search doesn't let you use `OR` for `created:` and `closed:` in the same query, but that's okay, we can put the results together ourselves.\nEach result has an `$items` field that contains a list of issues/pull requests,\neach with a whole bunch of fields.\nI'll use `purrr` to map over the issues and extract the fields I care about.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(purrr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nwork_data <-\n  c(created$items, closed$items) |>\n  map(function(issue) {\n    tibble(\n      repository = sub(\"https://api.github.com/repos/\", \"\", issue$repository_url),\n      title = issue$title,\n      created_at = issue$created_at,\n      closed_at = issue$closed_at,\n      url = issue$html_url,\n      state = issue$state,\n      is_issue = is.null(issue$pull_request),\n      body = issue$body\n    )\n  }) |>\n  list_rbind()\n\nwork_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 83 × 8\n   repository              title created_at url   state is_issue body  closed_at\n   <chr>                   <chr> <chr>      <chr> <chr> <lgl>    <chr> <chr>    \n 1 r-lib/systemfonts       Repo… 2025-01-2… http… open  TRUE     \"A s… <NA>     \n 2 rstudio/shiny           fix(… 2025-01-2… http… open  FALSE    \"Thi… <NA>     \n 3 rstudio/chromote        v0.4… 2025-01-2… http… open  FALSE    \"For… <NA>     \n 4 rstudio/chromote        Rele… 2025-01-2… http… open  TRUE     \"Pre… <NA>     \n 5 quarto-ext/bluesky-com… refa… 2025-01-2… http… open  FALSE    \"Fix… <NA>     \n 6 quarto-ext/bluesky-com… Add … 2025-01-2… http… open  TRUE     \"The… <NA>     \n 7 rstudio/bslib           Bran… 2025-01-2… http… clos… FALSE    \"Fix… 2025-01-…\n 8 rstudio/bslib           chor… 2025-01-2… http… clos… FALSE    \"Mov… 2025-01-…\n 9 rstudio/bslib           Bran… 2025-01-2… http… clos… TRUE     \"The… 2025-01-…\n10 quarto-dev/quarto-cli   All … 2025-01-2… http… open  TRUE     \"###… <NA>     \n# ℹ 73 more rows\n```\n\n\n:::\n:::\n\n\n\nNow that I have a nice rectangular summary of my work over the last week,\nI'll organize it a bit to differentiate between my Posit/Shiny work\nand my personal open-source projects.\nI'll also add an `action` column indicating the primary action this week.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwork_data <-\n  work_data |>\n  mutate(\n    personal = grepl(\"(^gadenbuie)|bluesky-comments\", repository),\n    action_at = coalesce(closed_at, created_at),\n    action = recode(state, open = \"opened\"),\n    action = paste(action, if_else(is_issue, \"an issue\", \"a PR\")),\n  ) |>\n  arrange(personal, repository, is_issue, action_at)\n\n# Preview the work data\nwork_data |>\n  select(repository, action, action_at, title)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 83 × 4\n   repository                  action          action_at            title       \n   <chr>                       <chr>           <chr>                <chr>       \n 1 posit-dev/air               opened an issue 2025-01-17T19:11:42Z Try to brea…\n 2 posit-dev/air               opened an issue 2025-01-17T20:11:24Z Empty or sh…\n 3 posit-dev/air               closed an issue 2025-01-21T19:56:11Z Selectively…\n 4 posit-dev/air               closed an issue 2025-01-21T19:56:11Z Selectively…\n 5 posit-dev/brand-yml         closed a PR     2025-01-21T17:32:35Z docs: Add g…\n 6 posit-dev/brand-yml         closed a PR     2025-01-21T17:32:35Z docs: Add g…\n 7 posit-dev/brand-yml         closed a PR     2025-01-21T19:58:30Z docs: Use G…\n 8 posit-dev/brand-yml         closed a PR     2025-01-21T19:58:30Z docs: Use G…\n 9 posit-dev/py-shiny          opened an issue 2025-01-21T16:25:43Z `render.Dat…\n10 posit-dev/py-shiny-validate closed a PR     2025-01-22T17:28:32Z fix: Disabl…\n# ℹ 73 more rows\n```\n\n\n:::\n:::\n\n\n\nRectangular data FTW!\nThis is great, I have a nice table of all the things I've done this week.\nBut it's still a bit hard to read to make sense of it all.\nWe've reached the \"I did some stuff and it's in this tibble\" stage,\nwhich is admittedly a little better than \"I did some stuff\".\n\nWhat I'd love to do is give this to a robot assistant\nwho could read it all for me\nand _they_ could summarize my week,\nor at least get me started on a summary.\nSounds like a great task for a large language model!\n\n## Hiring my executive robot assistant\n\nNow we get to the fun part![^fun-part]\nWe're going to use the newly released [ellmer package](https://ellmer.tidyverse.org)\nto send my work summary to a large language model\nand ask it to summarize my week.\n\n[^fun-part]: I know what you're thinking and you're totally right. The **fun part** was getting all this data in a nice tidy tibble! Think of all the things we could do with that tibble. But dplyr isn't the new hotness and this blog needs to meet its buzz word quota, so we're forced to do something extra.\n\nIf you're following along at home,\nhere's where you'll need to pause and get yourself set up.\nFirst, install ellmer:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"ellmer\")\n```\n:::\n\n\n\nSecond, you'll need access to an LLM.\nThe coolest part of ellmer is that it supports a wide range of LLM providers,\nwhich are all listed in the [Chatbots reference](https://ellmer.tidyverse.org/reference/index.html#chatbots).\nTo use your favorite LLM,\nyou'll need to set up an account and get an API key.\nBut once you've completed that step,\nswitching between LLMs is as easy as changing out the `chat_*()` function that you use to start the chat.\n\nFor this demo, I'll use Anthropic with [`chat_claude()`](https://ellmer.tidyverse.org/reference/chat_claude.html).\nThe help page for `chat_claude()` includes instructions for setting up a developer account, which I've already followed.\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ellmer)\n\nchat <- chat_claude()\n```\n:::\n\n\n\nIf you prefer OpenAI's ChatGPT, you'd use [`chat_openai()`](https://ellmer.tidyverse.org/reference/chat_openai.html),\nor if you have [ollama](https://ollama.com/) running a local model\nyou'd use [`chat_ollama()`](https://ellmer.tidyverse.org/reference/chat_ollama.html).\n\nOnce you have a `chat` object, you can use the `$chat()` method to talk to the LLM.\nThe answers are streamed back to your R session and the chat is cumulative, so you can keep \"talking\".\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchat$chat(\"Say hello three languages.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHere's \"hello\" in three languages:\n\n1. English: Hello\n2. Spanish: Hola\n3. French: Bonjour\n```\n\n\n:::\n\n```{.r .cell-code}\nchat$chat(\"Say goodbye in those same languages.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHere's \"goodbye\" in those same languages:\n\n1. English: Goodbye\n2. Spanish: Adiós\n3. French: Au revoir\n```\n\n\n:::\n:::\n\n\n\nOur final task is to give the LLM all of the information we've collected from my GitHub issues\nand then to convince it to summarize it all in a meaningful way.\nI can further refine this task into three distinct components:\n\n1. Use a **system prompt** to vibe check the LLM.\n   System prompts are a good place to store initial instructions\n   that the LLM uses a sort of \"light instruction set\",\n   like a sort of zero-th prompt.\n\n   I'll use the system prompt to convince the LLM that it's my assistant.\n\n2. Turn the work summary into something a little bit more readable\n   that I can send to the LLM as part of the user prompt.\n\n3. Write up a user prompt that gets the LLM to do what I want\n   (most of the time).\n\n### System prompt\n\nThis is the system prompt that I settled on:\n\n\n\n::: {.cell filename='_prompt-system.md'}\n\n```{.markdown .cell-code}\nYou are an assistant who keeps track of the work I do each week. Your goal is to\nfaithfully and accurately report the work I do to my boss while making sure that\nthe impact of my work is clearly articulated. You are clear and concise and use\ndirect language, preferring to match the terminology and phrases that I use. You\nrespond directly without any preamble.\n\n```\n:::\n\n\n\nThe goal is to try to help the LLM align with the _vibe_ of the task:\n\n* It needs to summarize my work\n* Please don't make things up\n* None of that \"Sure, here's your summary of...\" fluff.\n\n### Work context\n\nThe next step is to get the work summary into a format that's easier for LLMs to work with.\nThere's a good chance that I could just throw CSV or a JSON of the `work_data` table at it, but I think I'll get better answers if I format it like text.\nNotice that I've already grouped projects together\nand I've tried to make sure relevant parts of the work data are close to each other.\nI'm going off vibes with this,\nbut I expect I'll get better summaries with a little pre-work\nthan if I'd ask the LLM to make connections at great distance across the text.\n\nFor this task, I'll turn to my favorite string templating package,\n[epoxy](https://pkg.garrickadenbuie.com/epoxy).\nI've created a template that I'll apply to each row of `work_data`:\n\n\n\n::: {.cell filename='_prompt-work-summary.md'}\n\n```{.markdown .cell-code}\n---\nengine: epoxy\n---\n\n<work-item>\nAction: {action}\nTitle: {title}\nType: {ifelse(personal, 'Personal Work', 'Professional Work')}\nLink: {url}\n\n<work-description>\n{body}\n</work-description>\n</work-item>\n\n```\n:::\n\n\n\nI can take `work_data` and pass it to `epoxy::epoxy_use_file()`\nto fill in the templating bits with the values from `work_data`,\none row at a time.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(epoxy)\n\nwork_data |>\n  slice(15) |>\n  epoxy_use_file(\"_prompt-work-summary.md\", .engine)\n```\n:::\n\n\n\nHere's an example of a single work summary using the 15th row.\n\n`````html\n<work-item>\nAction: closed a PR\nTitle: chore: Add `done.` signal when package installation is completed\nType: Professional Work\nLink: https://github.com/posit-dev/shinylive/pull/195\n\n<work-description>\nIn #194 we went back to calling\n\n```patch\n- micropip.install(pkg_name)\n+ micropip.install(req)\n```\n\nbecause otherwise we lose information like the requested package version.\n\nI thought this broke installation of package extras because it appeared that `micropip.install()` doesn't actually install the packages listed in an extra. \n\nIt turns out that extras aren't installed when the main package is already installed. In other words, `micropip.install(\"shiny[theme]\")` will _not install_ `brand_yml` and `libsass` because `shiny` is already installed, but `micropip.install(\"httpx[socks]\")` _will install_ `socksio` along with `httpx` because `httpx` isn't initially installed.\n\nSo in the end, this PR mostly just adjusts a few comments and improves the UX of the console message. As [pointed out on Discord](https://discord.com/channels/1109483223987277844/1318357786379161731/1318536291750641674) the messages say `Installing {req} ...` and the last message seems to hang. This PR adds `done` to each line when the package install finishes.\n</work-description>\n</work-item>\n`````\n\nNotice that I've used `<work-item>` and `<work-description>` pseudo-tags\nto give the text a bit of structure.\n\nNow we can put the work summary together into a single text string.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwork_summary <-\n  work_data |>\n  epoxy_use_file(\"_prompt-work-summary.md\", .collapse = \"\\n\\n\")\n```\n:::\n\n\n\n### The user prompt\n\nThe last step is to give the LLM some instructions about what I want it to write.\nAgain, I'll use a prompt file for this task.\n\n\n\n::: {.cell filename='_prompt.md'}\n\n```{.markdown .cell-code}\nSummarize my work over the last week for a team update in prose.\nFocus more on pull requests; issues are less important and just provide context.\nWrite in prose with full paragraphs.\nFocus on the user impact of the work first and secondarily on the internal impact of dev changes.\n\nSummarize the work using markdown under two headings:\n'## Professional Work' and '## Personal Work'.\nInclude direct links to any mentioned pull requests using markdown syntax.\nUse the first-person and try to sound like me.\n\n<work-summary>\n{{ work_summary }}\n</work-summary>\n\n```\n:::\n\n\n\nFor this prompt, I'll use `ellmer::interpolate_file()`,\nwhich works a lot like `epoxy_use_file()` but comes built-in with ellmer.\nIt reads in a file and interpolates the `{{ expr }}` strings,\nwhich in this case will inject the `work_summary` into the prompt.\n\n## The final result\n\nFinally, we're ready to put it all together and find out if it works!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchat <- chat_claude(\n  system_prompt = readLines(\"_prompt-system.md\")\n)\n\nwork_summarized <-\n  chat$chat(interpolate_file(\"_prompt.md\"), echo=FALSE)\n\n# I'm caching the result so I don't rack up costs while writing this post\nsaveRDS(chat, \"chat.rds\")\n```\n:::\n\n\n\nI've set `echo=FALSE` so that I can store the result\nand show it to you with a big reveal,\nbut by default the response streams right into your console.\n\n\n\n::: {.cell}\nSending my whole work summary plus the prompt took about\n23,297 tokens\nand Claude responded with\n392 tokens.\nUsing [LLM Price Check](https://llmpricecheck.com/calculator/),\n25k input tokens and 500k output tokens,\nends up costing about $0.08 per call to `claude-3-sonnet`\nor about $0.13 per call to `gpt-4o`.\n:::\n\n::: {.cell}\n\n:::\n\n\n\nHere's the final output!\nellmer has a really neat feature that you can turn any part of the conversation into markdown\nwith `contents_markdown()`.\nI've used code like what's listed below to embed the response here in this post.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchat$last_turn() |>\n  contents_markdown()\n```\n:::\n\n\n\n::: {.callout-tip title=\"My Work Summary\" collapse=\"false\"}\n\n## Professional Work\n\nThis week I made significant contributions across multiple projects, improving functionality and user experience. In py-shinylive, I fixed Windows compatibility by ensuring path separators are consistent, converting backslashes to forward slashes. I also improved the user experience by adding completion signals when package installations finish.\n\nI worked on several important features in bslib, focusing on navbar compatibility and styling. I refactored the navbar functionality to better match Bootstrap 5's expectations around background color control and dark/light modes. I also fixed an issue where empty nav links in `nav_panel_hidden()` were incorrectly getting padding.\n\nIn chromote, I made several improvements to enhance debugging and user experience. I added a `chromote_info()` helper function to provide useful system and Chrome information, similar to `session_info()`. I also improved crash handling by setting a user-specific crash dump directory.\n\nFor the air code formatter, I opened several issues to improve functionality around line breaking and braced expression formatting. This included suggestions for selective format suppression and improved handling of empty or short braced expressions.\n\n## Personal Work\n\nI made significant contributions to the bluesky-comments Quarto extension, focusing on improving the user experience and adding new features. The main improvements include:\n\n- Refined content labelling UX to be more intuitive and match Bluesky's native behavior\n- Added support for displaying images and GIFs in comments\n- Implemented configurable comment thread depth with the `n-show-depth` option\n- Simplified the extension's documentation and improved its organization\n- Started work on CSS improvements to ensure consistent naming and avoid potential conflicts\n\nI also updated my shrtcts package to make it easier to run shortcuts by name when using `.shrtcts.R`, which enables better workflow automation in Positron.\n\n:::\n\n\n\n```{=html}\n<style>\n.callout h2 {\n  margin-top: inherit;\n}\n</style>\n```\n\n\n\nI have to admit, that's not a bad summary of my week!\n\nWhat's cool about this approach is that I could continue to chat with Claude about my week,\nasking follow up questions to have it further remind me about what I did.\n\nAnother idea I have now is to run this script in a weekly cron job,\njust before our larger team meeting,\nto make sure that I know what I did this week when I get there.\nNot only that, but this would make a great running summary\nthat might come in handy for a performance review or promotion request.\n\n## Using local LLMS\n\nJust to prove that it's easy to switch models,\nI'm going to repeat the above process with a local model.\nThis is the same code as above,\nbut this time using\n[ollama](https://ollama.com/) running `llama3.1:8b` locally.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchat_llama <- chat_ollama(\n  system_prompt = readLines(\"_prompt-system.md\"),\n  model = \"llama3.1:8b\"\n)\n\nwork_summarized_llama <-\n  chat_llama$chat(\n    interpolate_file(\"_prompt.md\"),\n    echo=FALSE\n  )\n```\n:::\n\n\n\n::: {.callout-tip title=\"My Work Summary (llama3.1:8b)\" collapse=\"true\"}\n\nIt looks like a summary of GitHub issues related to the Quarto extension \"bluesky-comments\". I'll extract some information and answer your question.\n\nThe extension allows you to embed Bluesky comments in a Quarto document. These are interactive conversations that can be embedded directly within Markdown content.\n\nFrom the summary, we can see that various features were proposed or implemented for this extension, such as:\n\n1. Displaying attached images in posts\n2. Limiting nested comment levels\n3. Showing the contents of comments (which seems to be a work-in-progress)\n4. Adding CSS classes for better styling (a pending issue)\n\nThere's also some discussion around customizing the comments section, such as making it optional or adding a header with a customizable title (from YAML front matter).\n\n:::\n\nThe local model is quite a bit slower,\nbut I can run it on my M3 MacBook Air.\nThe summarization quality is nowhere near what I got from Claude, either,\nbut I didn't need to pay for the query or send my data to a third-party.\n(I also didn't spend much time on this, I probably could get better results with more prompt engineering.)\nAt the very least,\nthe local model is a great way to practice setting up the pipeline\nbefore I'm ready to pay for queries with Anthropic or others.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
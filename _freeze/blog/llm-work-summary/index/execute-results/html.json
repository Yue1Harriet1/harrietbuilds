{
  "hash": "822abfc862db1c61bd3fe2801e7e4432",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Remembering what I did this week with LLMs\ndescription: >\n  Using the GitHub API and `{ellmer}` to remember what I did this week.\ncategories:\n  - R\n  - LLMs\n  - Personal Data\n  - Productivity\n  - GitHub\n  - ellmer\nimage: calendar-scribbles.png\nimage-alt: >\n  A generative AI image of a wall calendar behind a desk. The calendar is a mess.\ndate: last-modified\n\neditor: {render-on-save: true}\n---\n\n::: {.cell}\n\n:::\n\n\n\n## The object impermanence of work\n\nMy least favorite meetings are status meetings.\nI love seeing the people I work with and hearing about what they've been up to,\nbut I hate trying to remember what the hell I've been working on.\n\nWhen I joined the Shiny team,\nwe'd have a short daily status meeting\nand you'd think it'd be easy to remember what I did in the last 24 hours.\nBut apparently, that's not how I live my life.\nI'm sure I thought of things to say,\nbut every daily update I felt like I'd say some variation of\n\"I did a bunch of stuff and now I'm... doing more stuff\".\n\nMy perception of how I work is in short bursts of deep focus time\npunctuated by many short breaks to answer questions\non Slack or GitHub issues\nor [the Shiny Discord server](https://discord.gg/yMGCamUMnS)\nor [Posit Community](https://community.posit.co)\nor any of the many other places that people post questions and sometimes code.\n\nBetween the deep focus work,\nI usually maintain one thread of maintenance tasks,\nlike babysitting a CI process,\nor making a small change to fix a bug,\nor [turning screenshots into reproducible bug reports](https://bsky.app/profile/grrrck.xyz/post/3lgdolbiqis25).\nAnd I also tend to have another more speculative thread going\nfor learning something new or playing with a new idea.\n\nI don't recommend my way of working to others.\nWhole books have been written just to say \"don't work like this\"!\nSoftware engineers rail against\n[context switching](https://contextkeeper.io/blog/the-real-cost-of-an-interruption-and-context-switching/),\n[interruptions](https://blog.stackblitz.com/posts/flow-state/),\nand [multitasking](https://www.apa.org/topics/research/multitasking).\n\nI compensate for the known perils of my work style in two ways.\nFirst, even if I do a lot of different things throughout the day,\nthere's generally only ever _one task_ that I'm working on at any given moment.\nThat task gets all the focus.\n\nSecond, I try to leave context behind in my work.\nIf I'm working on a pull request,\nthat means submitting the PR with enough description that I could\n[forget that I even wrote the PR](https://bsky.app/profile/grrrck.xyz/post/3lfslcgnoh225)\nand most of what I'd need to pick it back up is there in the description or comments.\nIf I had some code in front of me when I found the issue or was testing the PR,\nthat code ends up in the description.\nIf the context is recoverable, it's at least not lost forever in the switching.\n\nBut these two strategies combine in a strange way,\nresulting in the **object impermanence of work**:\n_if it's not in front of me, it doesn't exist_.\n\nI just don't keep a running inventory of what I was working on or what I did;\nI know that I can find everything I need to know from the open GitHub issues or PRs.\nWhich then are summarized in stand ups as... _stuff_.\n\n## What did I even do this week?\n\nIt turns out I _can_ answer the question of what I did this week,\nas long as I'm allowed to run some R code.\n\n\n\n::: {.cell}\n\n:::\n\n\n\nSince my entire life is lived on GitHub,\nI can get a pretty good sense of what I've been working on\nby looking at the PRs and issues I opened and closed in a week.\nEven better, this information is readily obtained from GitHub with a search like\n[`` author:gadenbuie created:>2025-01-17T00:00:00Z ``](https://github.com/search?q=author:gadenbuie%20created:%3E2025-01-17T00:00:00Z) or\n[`` author:gadenbuie closed:>2025-01-17T00:00:00Z ``](https://github.com/search?q=author:gadenbuie%20closed:%3E2025-01-17T00:00:00Z).\nAnd I can create those search queries with R:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nauthor <- \"gadenbuie\"\ndate <- Sys.Date() - 7\nsearch_created <- sprintf(\"author:%s created:>%sT00:00:00Z\", author, date)\nsearch_closed <- sprintf(\"author:%s closed:>%sT00:00:00Z\", author, date)\n```\n:::\n\n\n\nBut of course I don't want to have to actually click around on GitHub to remember what I was doing.\nWhat I want is some rectangular data that I can work on in R.\n\nEnter the [`gh` package](https://gh.r-lib.org),\na package that lets you interact with the GitHub API.\nWe can use `gh` to run the created and closed search queries\nand collect all of the results.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gh)\n\ncreated <- gh(\"/search/issues\", q = search_created, .limit = Inf)\nclosed <- gh(\"/search/issues\", q = search_closed, .limit = Inf)\n```\n:::\n\n\n\nWhy not do this in one single API request?\nUnfortunately, GitHub search doesn't let you use `OR` for `created:` and `closed:` in the same query, but that's okay, we can put the results together ourselves.\nEach result has an `$items` field that contains a list of issues/pull requests,\neach with a whole bunch of fields.\nI'll use `purrr` to map over the issues and extract the fields I care about.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(purrr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\nwork_data <-\n  c(created$items, closed$items) |>\n    map(function(issue) {\n      tibble(\n        repository = sub(\n          \"https://api.github.com/repos/\",\n          \"\",\n          issue$repository_url\n        ),\n        title = issue$title,\n        created_at = issue$created_at,\n        closed_at = issue$closed_at,\n        url = issue$html_url,\n        state = issue$state,\n        is_issue = is.null(issue$pull_request),\n        body = issue$body\n      )\n    }) |>\n    list_rbind()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwork_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 83 × 8\n   repository              title created_at url   state is_issue body  closed_at\n   <chr>                   <chr> <chr>      <chr> <chr> <lgl>    <chr> <chr>    \n 1 r-lib/systemfonts       Repo… 2025-01-2… http… open  TRUE     \"A s… <NA>     \n 2 rstudio/shiny           fix(… 2025-01-2… http… open  FALSE    \"Thi… <NA>     \n 3 rstudio/chromote        v0.4… 2025-01-2… http… open  FALSE    \"For… <NA>     \n 4 rstudio/chromote        Rele… 2025-01-2… http… open  TRUE     \"Pre… <NA>     \n 5 quarto-ext/bluesky-com… refa… 2025-01-2… http… open  FALSE    \"Fix… <NA>     \n 6 quarto-ext/bluesky-com… Add … 2025-01-2… http… open  TRUE     \"The… <NA>     \n 7 rstudio/bslib           Bran… 2025-01-2… http… clos… FALSE    \"Fix… 2025-01-…\n 8 rstudio/bslib           chor… 2025-01-2… http… clos… FALSE    \"Mov… 2025-01-…\n 9 rstudio/bslib           Bran… 2025-01-2… http… clos… TRUE     \"The… 2025-01-…\n10 quarto-dev/quarto-cli   All … 2025-01-2… http… open  TRUE     \"###… <NA>     \n# ℹ 73 more rows\n```\n\n\n:::\n:::\n\n\n\nNow that I have a nice rectangular summary of my work over the last week,\nI'll organize it a bit to differentiate between my Posit/Shiny work\nand my personal open-source projects.\nI'll also add an `action` column indicating the primary action this week.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwork_data <-\n  work_data |>\n    mutate(\n      personal = grepl(\"(^gadenbuie)|bluesky-comments\", repository),\n      action_at = coalesce(closed_at, created_at),\n      action = recode(state, open = \"opened\"),\n      action = paste(action, if_else(is_issue, \"an issue\", \"a PR\")),\n    ) |>\n    arrange(personal, repository, is_issue, action_at)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Preview the work data\nwork_data |>\n  select(repository, action, action_at, title)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 83 × 4\n   repository                  action          action_at            title       \n   <chr>                       <chr>           <chr>                <chr>       \n 1 posit-dev/air               opened an issue 2025-01-17T19:11:42Z Try to brea…\n 2 posit-dev/air               opened an issue 2025-01-17T20:11:24Z Empty or sh…\n 3 posit-dev/air               closed an issue 2025-01-21T19:56:11Z Selectively…\n 4 posit-dev/air               closed an issue 2025-01-21T19:56:11Z Selectively…\n 5 posit-dev/brand-yml         closed a PR     2025-01-21T17:32:35Z docs: Add g…\n 6 posit-dev/brand-yml         closed a PR     2025-01-21T17:32:35Z docs: Add g…\n 7 posit-dev/brand-yml         closed a PR     2025-01-21T19:58:30Z docs: Use G…\n 8 posit-dev/brand-yml         closed a PR     2025-01-21T19:58:30Z docs: Use G…\n 9 posit-dev/py-shiny          opened an issue 2025-01-21T16:25:43Z `render.Dat…\n10 posit-dev/py-shiny-validate closed a PR     2025-01-22T17:28:32Z fix: Disabl…\n# ℹ 73 more rows\n```\n\n\n:::\n:::\n\n\n\nRectangular data FTW!\nThis is great, I have a nice table of all the things I've done this week.\nBut it's still a bit hard to read to make sense of it all.\nWe've reached the \"I did some stuff and it's in this tibble\" stage,\nwhich is admittedly a little better than \"I did some stuff\".\n\nWhat I'd love to do is give this to a robot assistant\nwho could read it all for me\nand _they_ could summarize my week,\nor at least get me started on a summary.\nSounds like a great task for a large language model!\n\n## Hiring my executive robot assistant\n\nNow we get to the fun part![^fun-part]\nWe're going to use the newly released [ellmer package](https://ellmer.tidyverse.org)\nto send my work summary to a large language model\nand ask it to summarize my week.\n\n[^fun-part]: I know what you're thinking and you're totally right. The **fun part** was getting all this data in a nice tidy tibble! Think of all the things we could do with that tibble. But dplyr isn't the new hotness and this blog needs to meet its buzz word quota, so we're forced to do something extra.\n\nIf you're following along at home,\nhere's where you'll need to pause and get yourself set up.\nFirst, install ellmer:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"ellmer\")\n```\n:::\n\n\n\nSecond, you'll need access to an LLM.\nThe coolest part of ellmer is that it supports a wide range of LLM providers,\nwhich are all listed in the [Chatbots reference](https://ellmer.tidyverse.org/reference/index.html#chatbots).\nTo use your favorite LLM,\nyou'll need to set up an account and get an API key.\nBut once you've completed that step,\nswitching between LLMs is as easy as changing out the `chat_*()` function that you use to start the chat.\n\nFor this demo, I'll use Anthropic with [`chat_claude()`](https://ellmer.tidyverse.org/reference/chat_claude.html).\nThe help page for `chat_claude()` includes instructions for setting up a developer account, which I've already followed.\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ellmer)\n\nchat <- chat_claude()\n```\n:::\n\n\n\nIf you prefer OpenAI's ChatGPT, you'd use [`chat_openai()`](https://ellmer.tidyverse.org/reference/chat_openai.html),\nor if you have [ollama](https://ollama.com/) running a local model\nyou'd use [`chat_ollama()`](https://ellmer.tidyverse.org/reference/chat_ollama.html).\n\nOnce you have a `chat` object, you can use the `$chat()` method to talk to the LLM.\nThe answers are streamed back to your R session and the chat is cumulative, so you can keep \"talking\".\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchat$chat(\"Say hello three languages.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHere's \"hello\" in three languages:\n\n1. English: Hello\n2. Spanish: Hola\n3. French: Bonjour\n```\n\n\n:::\n\n```{.r .cell-code}\nchat$chat(\"Say goodbye in those same languages.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHere's \"goodbye\" in those same languages:\n\n1. English: Goodbye\n2. Spanish: Adiós\n3. French: Au revoir\n```\n\n\n:::\n:::\n\n\n\nOur final task is to give the LLM all of the information we've collected from my GitHub issues\nand then to convince it to summarize it all in a meaningful way.\nI can further refine this task into three distinct components:\n\n1. Use a **system prompt** to vibe check the LLM.\n   System prompts are a good place to store initial instructions\n   that the LLM uses a sort of \"light instruction set\",\n   like a sort of zero-th prompt.\n\n   I'll use the system prompt to convince the LLM that it's my assistant.\n\n2. Turn the work summary into something a little bit more readable\n   that I can send to the LLM as part of the user prompt.\n\n3. Write up a user prompt that gets the LLM to do what I want\n   (most of the time).\n\n### System prompt\n\nThis is the system prompt that I settled on:\n\n\n\n::: {.cell filename='_prompt-system.md'}\n\n```{.markdown .cell-code}\nYou are an assistant who keeps track of the work I do each week. Your goal is to\nfaithfully and accurately report the work I do to my boss while making sure that\nthe impact of my work is clearly articulated. You are clear and concise and use\ndirect language, preferring to match the terminology and phrases that I use. You\nrespond directly without any preamble.\n\n```\n:::\n\n\n\nThe goal is to try to help the LLM align with the _vibe_ of the task:\n\n* It needs to summarize my work\n* Please don't make things up\n* None of that \"Sure, here's your summary of...\" fluff.\n\n### Work context\n\nThe next step is to get the work summary into a format that's easier for LLMs to work with.\nThere's a good chance that I could just throw CSV or a JSON of the `work_data` table at it, but I think I'll get better answers if I format it like text.\nNotice that I've already grouped projects together\nand I've tried to make sure relevant parts of the work data are close to each other.\nI'm going off vibes with this,\nbut I expect I'll get better summaries with a little pre-work\nthan if I'd ask the LLM to make connections at great distance across the text.\n\nFor this task, I'll turn to my favorite string templating package,\n[epoxy](https://pkg.garrickadenbuie.com/epoxy).\nI've created a template that I'll apply to each row of `work_data`:\n\n\n\n::: {.cell filename='_prompt-work-summary.md'}\n\n```{.markdown .cell-code}\n---\nengine: epoxy\n---\n\n<work-item>\nAction: {action}\nTitle: {title}\nType: {ifelse(personal, 'Personal Work', 'Professional Work')}\nLink: {url}\n\n<work-description>\n{body}\n</work-description>\n</work-item>\n\n```\n:::\n\n\n\nI can take `work_data` and pass it to `epoxy::epoxy_use_file()`\nto fill in the templating bits with the values from `work_data`,\none row at a time.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(epoxy)\n\nwork_data |>\n  slice(15) |>\n  epoxy_use_file(\"_prompt-work-summary.md\", .engine)\n```\n:::\n\n\n\nHere's an example of a single work summary using the 15th row.\n\n`````html\n<work-item>\nAction: closed a PR\nTitle: chore: Add `done.` signal when package installation is completed\nType: Professional Work\nLink: https://github.com/posit-dev/shinylive/pull/195\n\n<work-description>\nIn #194 we went back to calling\n\n```patch\n- micropip.install(pkg_name)\n+ micropip.install(req)\n```\n\nbecause otherwise we lose information like the requested package version.\n\nI thought this broke installation of package extras because it appeared that `micropip.install()` doesn't actually install the packages listed in an extra. \n\nIt turns out that extras aren't installed when the main package is already installed. In other words, `micropip.install(\"shiny[theme]\")` will _not install_ `brand_yml` and `libsass` because `shiny` is already installed, but `micropip.install(\"httpx[socks]\")` _will install_ `socksio` along with `httpx` because `httpx` isn't initially installed.\n\nSo in the end, this PR mostly just adjusts a few comments and improves the UX of the console message. As [pointed out on Discord](https://discord.com/channels/1109483223987277844/1318357786379161731/1318536291750641674) the messages say `Installing {req} ...` and the last message seems to hang. This PR adds `done` to each line when the package install finishes.\n</work-description>\n</work-item>\n`````\n\nNotice that I've used `<work-item>` and `<work-description>` pseudo-tags\nto give the text a bit of structure.\n\nNow we can put the work summary together into a single text string.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwork_summary <-\n  work_data |>\n    epoxy_use_file(\"_prompt-work-summary.md\", .collapse = \"\\n\\n\")\n```\n:::\n\n\n\n### The user prompt\n\nThe last step is to give the LLM some instructions about what I want it to write.\nAgain, I'll use a prompt file for this task.\n\n\n\n::: {.cell filename='_prompt.md'}\n\n```{.markdown .cell-code}\nSummarize my work over the last week for a team update in prose.\nFocus more on pull requests; issues are less important and just provide context.\nWrite in prose with full paragraphs.\nFocus on the user impact of the work first and secondarily on the internal impact of dev changes.\n\nSummarize the work using markdown under two headings:\n'## Professional Work' and '## Personal Work'.\nInclude direct links to any mentioned pull requests using markdown syntax.\nUse the first-person and try to sound like me.\n\n<work-summary>\n{{ work_summary }}\n</work-summary>\n\n```\n:::\n\n\n\nFor this prompt, I'll use `ellmer::interpolate_file()`,\nwhich works a lot like `epoxy_use_file()` but comes built-in with ellmer.\nIt reads in a file and interpolates the `{{ expr }}` strings,\nwhich in this case will inject the `work_summary` into the prompt.\n\n## The final result\n\nFinally, we're ready to put it all together and find out if it works!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchat <- chat_claude(\n  system_prompt = readLines(\"_prompt-system.md\")\n)\n\nwork_summarized <-\n  chat$chat(interpolate_file(\"_prompt.md\"), echo = FALSE)\n\n# I'm caching the result so I don't rack up costs while writing\nsaveRDS(chat, \"chat.rds\")\n```\n:::\n\n\n\nI've set `echo=FALSE` so that I can store the result\nand show it to you with a big reveal,\nbut by default the response streams right into your console.\n\n\n\n::: {.cell}\nSending my whole work summary plus the prompt took about\n23,297 tokens\nand Claude responded with\n594 tokens.\nUsing [LLM Price Check](https://llmpricecheck.com/calculator/),\n25k input tokens and 500k output tokens,\nends up costing about $0.08 per call to `claude-3-sonnet`\nor about $0.13 per call to `gpt-4o`.\n:::\n\n::: {.cell}\n\n:::\n\n\n\nHere's the final output!\nellmer has a really neat feature that you can turn any part of the conversation into markdown\nwith `contents_markdown()`.\nI've used code like what's listed below to embed the response here in this post.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchat$last_turn() |>\n  contents_markdown()\n```\n:::\n\n\n\n::: {.callout-tip title=\"My Work Summary\" collapse=\"false\"}\n\n## Professional Work\n\nI made significant improvements across multiple projects this week. In bslib, I improved navbar compatibility for Bootstrap 5 and light/dark modes through [PR #1145](https://github.com/rstudio/bslib/pull/1145), which brought bslib in line with Bootstrap 5's expectations around navbar background color control while adding appropriate controls for various scenarios. I also contributed to better code organization by moving navbar options to a dedicated file through [PR #1173](https://github.com/rstudio/bslib/pull/1173).\n\nIn chromote, I enhanced the developer experience by adding the `chromote_info()` helper function via [PR #190](https://github.com/rstudio/chromote/pull/190) to provide useful system and configuration information. I also improved session management by setting user-specific crash dump directories in [PR #194](https://github.com/rstudio/chromote/pull/194).\n\nFor shinylive and Shiny, I made several improvements to enhance stability and user experience. I fixed Windows path handling in shinylive through [PR #47](https://github.com/posit-dev/py-shinylive/pull/47) and improved the insertion of nav items in Shiny with [PR #4179](https://github.com/rstudio/shiny/pull/4179).\n\nI also worked on improving code formatting across our repositories by setting up automated R code formatting with `air` through [PR #43](https://github.com/rstudio/shiny-workflows/pull/43) in the shiny-workflows repository.\n\n## Personal Work\n\nI made substantial contributions to the bluesky-comments Quarto extension, implementing several key features and improvements:\n\n- Enhanced content label handling and UI through [PR #29](https://github.com/quarto-ext/bluesky-comments/pull/29)\n- Added support for images and GIFs in [PR #40](https://github.com/quarto-ext/bluesky-comments/pull/40)\n- Implemented nested comment depth control via [PR #41](https://github.com/quarto-ext/bluesky-comments/pull/41)\n- Improved header configuration options in [PR #39](https://github.com/quarto-ext/bluesky-comments/pull/39)\n\nI also enhanced shrtcts to support better workflow integration in Positron through [PR #26](https://github.com/gadenbuie/shrtcts/pull/26), adding `shortcut_run()` and `shortcut_get()` functions.\n\n:::\n\n\n\n```{=html}\n<style>\n.callout h2 {\n  margin-top: inherit;\n}\n</style>\n```\n\n\n\nI have to admit, that's not a bad summary[^links] of my week!\n\n[^links]: Does the summary have links to the PRs? This one was hit-or-miss. Sometimes Claude includes these and sometimes they're omitted. I'm sure a little more prompt massaging could improve the consistency.\n\nWhat's cool about this approach is that I could continue to chat with Claude about my week,\nasking follow up questions to have it further remind me about what I did.\n\nAnother idea I have now is to run this script in a weekly cron job,\njust before our larger team meeting,\nto make sure that I know what I did this week when I get there.\nNot only that, but this would make a great running summary\nthat might come in handy for a performance review or promotion request.\n\n## Using local LLMS\n\nJust to prove that it's easy to switch models,\nI'm going to repeat the above process with a local model.\nThis is the same code as above,\nbut this time using\n[ollama](https://ollama.com/) to run\n[phi4](https://ollama.com/library/phi4),\na new 14B parameter model from Microsoft.\n\nBefore I show you the results, I have to admit something:\nthey're ... not stellar.\nI tried a few other local models,\nand `phi4` generally did the best.\nA 14B parameter model isn't the _smallest model_ on the block,\nbut it's still pretty small.\nBy comparison,\nClaude 3.5 Sonnet has roughly\n[175 billion parameters](https://aiexpjourney.substack.com/p/the-number-of-parameters-of-gpt-4o).\nThe smaller parameter models have almost no chance of acing this task.\n\nI also found that if I tried to summarize\nboth professional and personal work in the same chat,\nthe local models would get hung up on the personal work\nand would end up summarizing only that.\nAfter playing around with it for a bit,\nI decided to focus just on the professional work summary,\nwhere `phi4` does okay.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprofessional_work <-\n  work_data |>\n    filter(!personal) |>\n    epoxy_use_file(\"_prompt-work-summary.md\", .collapse = \"\\n\\n\")\n\nchat_llama <- chat_ollama(\n  system_prompt = brio::read_file(\"_prompt-system.md\"),\n  model = \"phi4\",\n  api_args = list(temperature = 0.2)\n)\n\nwork_summarized_llama <-\n  chat_llama$chat(\n    interpolate_file(\n      \"_prompt.md\",\n      work_summary = professional_work\n    ),\n    echo = FALSE\n  )\n```\n:::\n\n\n\n\n::: {.callout-tip title=\"My Work Summary (phi4)\" collapse=\"true\"}\n\n### Summary of Recent Work\n\n#### Professional Activities:\n\n1. **Shiny Core CI Snapshots Update:**\n   - Updated snapshots due to Chrome v132's deprecation of `--headless=old` mode.\n   - Chromote now uses the new `--headless` mode by default (see [Chromote PR #189](https://github.com/rstudio/chromote/pull/189)).\n   - Reviewed all updated snapshots; no behavior changes detected, except for potential issues with absolute positioning in panel layouts (`027-absolutely-positioned-panels`).\n\n2. **Shiny Workflows Enhancement:**\n   - Introduced a new composite workflow `rstudio/shiny-workflows/format-r-code` to format R code using `air`.\n   - This workflow is part of the routine process, activated via `inputs.format-r-code`, which defaults to `false`.\n   - Users can enable it by setting `format-r-code: true` in their configuration.\n   - Additional inputs for this workflow include:\n     - `version`: Specifies the version of `air` (default: `latest`).\n     - `check`: When set to `\"true\"`, checks if files change without committing (`air format --check`).\n     - `path`: Defines paths for formatting, defaulting to all R files in the repository.\n   - Demonstrated usage in the bslib project ([bslib PR #1167](https://github.com/rstudio/bslib/pull/1167)).\n\n3. **Shiny Core CI Snapshots Update (Duplicate Entry):**\n   - Reiterated update due to Chrome v132 changes, with similar outcomes and considerations as previously noted.\n\n#### Open Source Contributions:\n\n1. **Shiny PR #4952:**\n   - Addressed a bug where `shiny::runApp()` would fail if the app directory was not writable.\n   - Implemented a fix that allows apps to run in read-only directories by copying files into a temporary location for execution.\n\n2. **Shiny Core CI Snapshots Update (Duplicate Entry):**\n   - Repeated update details regarding Chrome v132 and Chromote's headless mode transition, with no additional changes noted beyond the initial entry.\n\nThese activities reflect ongoing efforts to maintain and enhance the Shiny ecosystem, ensuring compatibility with new browser versions and improving developer workflows.\n\n:::\n\n\nThe local model takes quite a bit longer to provide a response,\nbut I _can_ run it on my M3 MacBook Air.\nThe summarization quality is nowhere near what I got from Claude, either,\nbut I didn't need to pay for the query or send my data to a third-party.\n(I also didn't spend much time on this, I probably could get better results with more prompt engineering.)\nAt the very least,\nthe local model is a great way to practice setting up the pipeline\nbefore I'm ready to pay for queries with Anthropic or others.\n\n## Final script\n\nOne last thing:\nhere's all of the code from the meandering blog post above in one place,\nwith all of the prompts inlined.\nAmazingly, its less than 100 lines of code.\n\n```r\n# Query issues and pull requests from this week -------------------------------\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(gh)\n\nauthor <- \"gadenbuie\"\nstart_date <- Sys.Date() - 7\n\nsearch_created <- sprintf(\"author:%s created:>%sT00:00:00Z\", author, start_date)\nsearch_closed <- sprintf(\"author:%s closed:>%sT00:00:00Z\", author, start_date)\n\ncreated <- gh(\"/search/issues\", q = search_created, .limit = Inf)\nclosed <- gh(\"/search/issues\", q = search_closed, .limit = Inf)\n\nwork_data <-\n  c(created$items, closed$items) |>\n    map(function(issue) {\n      tibble(\n        repository = sub(\n          \"https://api.github.com/repos/\",\n          \"\",\n          issue$repository_url\n        ),\n        title = issue$title,\n        created_at = issue$created_at,\n        closed_at = issue$closed_at,\n        url = issue$html_url,\n        state = issue$state,\n        is_issue = is.null(issue$pull_request),\n        body = issue$body\n      )\n    }) |>\n    list_rbind() |>\n      mutate(\n        personal = grepl(\"(^gadenbuie)|bluesky-comments\", repository),\n        action_at = coalesce(closed_at, created_at),\n        action = recode(state, open = \"opened\"),\n        action = paste(action, if_else(is_issue, \"an issue\", \"a PR\")),\n      ) |>\n      arrange(personal, repository, is_issue, action_at)\n\n# Prepare the work data summary -----------------------------------------------\nlibrary(epoxy)\n\nwork_summary <-\n  epoxy(\n    \"<work-item>\n    Action: {action}\n    Title: {title}\n    Type: {ifelse(personal, 'Personal Work', 'Professional Work')}\n    Link: {url}\n\n    <work-description>\n    {body}\n    </work-description>\n    </work-item>\",\n    .collapse = \"\\n\\n\"\n  )\n\n# Summarize with the LLM ------------------------------------------------------\nlibrary(ellmer)\n\nchat <- chat_claude(\n  system_prompt = epoxy(\n    \"You are an assistant who keeps track of the work I do each week. Your goal is to\n    faithfully and accurately report the work I do to my boss while making sure that\n    the impact of my work is clearly articulated. You are clear and concise and use\n    direct language, preferring to match the terminology and phrases that I use. You\n    respond directly without any preamble.\"\n  )\n)\n\nchat$chat(\n  interpolate(\n    \"Summarize my work over the last week for a team update in prose.\n    Focus more on pull requests; issues are less important and just provide context.\n    Write in prose with full paragraphs.\n    Focus on the user impact of the work first and secondarily on the internal impact of dev changes.\n\n    Summarize the work using markdown under two headings:\n    '## Professional Work' and '## Personal Work'.\n    Include direct links to any mentioned pull requests using markdown syntax.\n    Use the first-person and try to sound like me.\n\n    <work-summary>\n    {{ work_summary }}\n    </work-summary>\"\n  )\n)\n\n# To get the LLM response as markdown:\ncontents_markdown(chat$last_turn())\n\n```\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}